#!/usr/bin/env python3
"""
æŸ¥çœ‹ CoachMe æ•°æ®é›†å†…å®¹çš„ç®€å•è„šæœ¬
ç”¨æ³•: python view_dataset.py
"""

import pickle
import torch
import json
from pathlib import Path

def view_pkl_dataset(pkl_path, show_details=True, max_samples=3):
    """
    æŸ¥çœ‹ PKL æ•°æ®é›†çš„å†…å®¹
    
    Args:
        pkl_path: pkl æ–‡ä»¶è·¯å¾„
        show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        max_samples: æœ€å¤šæ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
    """
    print(f"{'='*80}")
    print(f"ğŸ“¦ æŸ¥çœ‹æ•°æ®é›†: {pkl_path}")
    print(f"{'='*80}\n")
    
    # åŠ è½½æ•°æ®
    try:
        with open(pkl_path, 'rb') as f:
            dataset = pickle.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {e}")
        return
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
    print(f"   æ•°æ®ç±»å‹: {type(dataset)}")
    print(f"   æ ·æœ¬æ•°é‡: {len(dataset)}")
    print(f"   æ¯ä¸ªæ ·æœ¬ç±»å‹: {type(dataset[0]) if len(dataset) > 0 else 'N/A'}")
    print()
    
    if len(dataset) == 0:
        print("âš ï¸  æ•°æ®é›†ä¸ºç©ºï¼")
        return
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é”®
    first_item = dataset[0]
    print(f"ğŸ”‘ æ•°æ®å­—æ®µ:")
    for key in first_item.keys():
        value = first_item[key]
        if isinstance(value, torch.Tensor):
            print(f"   - {key:25s}: Tensor {value.shape} ({value.dtype})")
        elif isinstance(value, list):
            print(f"   - {key:25s}: List (é•¿åº¦: {len(value)})")
        elif isinstance(value, (int, float)):
            print(f"   - {key:25s}: {type(value).__name__} = {value}")
        else:
            print(f"   - {key:25s}: {type(value).__name__}")
    print()
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    
    # ç»Ÿè®¡åŠ¨ä½œç±»å‹
    if 'motion_type' in first_item:
        motion_types = {}
        for item in dataset:
            mt = item.get('motion_type', 'Unknown')
            motion_types[mt] = motion_types.get(mt, 0) + 1
        print(f"   åŠ¨ä½œç±»å‹åˆ†å¸ƒ:")
        for mt, count in motion_types.items():
            print(f"      - {mt}: {count} ä¸ªæ ·æœ¬")
    
    # ç»Ÿè®¡åºåˆ—é•¿åº¦
    if 'coordinates' in first_item or 'original_seq_len' in first_item:
        seq_lens = []
        for item in dataset:
            if 'original_seq_len' in item:
                seq_lens.append(item['original_seq_len'])
            elif 'coordinates' in item:
                seq_lens.append(len(item['coordinates']))
        
        if seq_lens:
            print(f"   åºåˆ—é•¿åº¦:")
            print(f"      - æœ€å°: {min(seq_lens)} å¸§")
            print(f"      - æœ€å¤§: {max(seq_lens)} å¸§")
            print(f"      - å¹³å‡: {sum(seq_lens)/len(seq_lens):.1f} å¸§")
    
    # ç»Ÿè®¡æ ‡ç­¾æ•°é‡
    if 'labels' in first_item:
        label_counts = [len(item['labels']) for item in dataset if 'labels' in item and item['labels']]
        if label_counts:
            print(f"   æ ‡ç­¾æ•°é‡:")
            print(f"      - æœ€å°: {min(label_counts)} æ¡")
            print(f"      - æœ€å¤§: {max(label_counts)} æ¡")
            print(f"      - å¹³å‡: {sum(label_counts)/len(label_counts):.1f} æ¡")
    
    print()
    
    # æ˜¾ç¤ºè¯¦ç»†æ ·æœ¬
    if show_details:
        print(f"{'='*80}")
        print(f"ğŸ“‹ æ ·æœ¬è¯¦æƒ… (å‰ {min(max_samples, len(dataset))} ä¸ª):")
        print(f"{'='*80}\n")
        
        for idx in range(min(max_samples, len(dataset))):
            item = dataset[idx]
            print(f"ã€æ ·æœ¬ {idx + 1}ã€‘")
            print(f"  è§†é¢‘åç§°: {item.get('video_name', 'N/A')}")
            print(f"  åŠ¨ä½œç±»å‹: {item.get('motion_type', 'N/A')}")
            
            if 'coordinates' in item:
                coords = item['coordinates']
                print(f"  åæ ‡æ•°æ®: {coords.shape} (å¸§æ•°Ã—ç‰¹å¾ç»´åº¦)")
                print(f"            min={coords.min():.3f}, max={coords.max():.3f}, mean={coords.mean():.3f}")
            
            if 'original_seq_len' in item:
                print(f"  åŸå§‹é•¿åº¦: {item['original_seq_len']} å¸§")
            
            if 'camera_view' in item:
                print(f"  æ‘„åƒæœºè§†è§’: {item['camera_view']}")
            
            # æ˜¾ç¤ºåˆ†æ®µä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            segment_keys = ['aligned_start_frame', 'aligned_end_frame', 'aligned_seq_len',
                           'error_start_frame', 'error_end_frame', 'error_seq_len',
                           'gt_start_frame', 'gt_end_frame', 'gt_seq_len']
            segments = {k: item[k] for k in segment_keys if k in item}
            if segments:
                print(f"  åˆ†æ®µä¿¡æ¯:")
                for k, v in segments.items():
                    print(f"    - {k}: {v}")
            
            # æ˜¾ç¤ºæ ‡ç­¾
            if 'labels' in item and item['labels']:
                print(f"  æ ‡ç­¾ ({len(item['labels'])} æ¡):")
                for i, label in enumerate(item['labels'][:2], 1):  # åªæ˜¾ç¤ºå‰2æ¡
                    label_preview = label[:80] + '...' if len(label) > 80 else label
                    print(f"    [{i}] {label_preview}")
                if len(item['labels']) > 2:
                    print(f"    ... è¿˜æœ‰ {len(item['labels']) - 2} æ¡æ ‡ç­¾")
            
            # æ˜¾ç¤ºå¢å¼ºæ ‡ç­¾
            if 'augmented_labels' in item and item['augmented_labels']:
                print(f"  å¢å¼ºæ ‡ç­¾: {len(item['augmented_labels'])} æ¡")
            
            print()
    
    # æä¾›å¯¼å‡ºé€‰é¡¹
    print(f"{'='*80}")
    print(f"ğŸ’¡ æç¤º:")
    print(f"   - ä½¿ç”¨ save_to_json=True å¯ä»¥å¯¼å‡ºä¸º JSON æŸ¥çœ‹")
    print(f"   - ä½¿ç”¨ max_samples å‚æ•°å¯ä»¥æŸ¥çœ‹æ›´å¤šæ ·æœ¬")
    print(f"{'='*80}\n")


def save_dataset_to_json(pkl_path, json_path):
    """
    å°† PKL æ•°æ®é›†å¯¼å‡ºä¸º JSONï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼Œä½†ä¸åŒ…å« Tensorï¼‰
    """
    with open(pkl_path, 'rb') as f:
        dataset = pickle.load(f)
    
    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
    json_data = []
    for item in dataset:
        json_item = {}
        for key, value in item.items():
            if isinstance(value, torch.Tensor):
                json_item[key] = {
                    "_type": "Tensor",
                    "shape": list(value.shape),
                    "dtype": str(value.dtype),
                    "min": float(value.min()),
                    "max": float(value.max()),
                    "mean": float(value.mean())
                }
            elif isinstance(value, list):
                json_item[key] = value
            else:
                json_item[key] = value
        json_data.append(json_item)
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²å¯¼å‡ºåˆ°: {json_path}")


if __name__ == "__main__":
    # æŸ¥çœ‹ BX_test.pkl
    pkl_file = "dataset/BX_test.pkl"
    
    if not Path(pkl_file).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {pkl_file}")
        print(f"ğŸ’¡ è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    else:
        # æŸ¥çœ‹æ•°æ®é›†
        view_pkl_dataset(pkl_file, show_details=True, max_samples=3)
        
        # å¯é€‰ï¼šå¯¼å‡ºä¸º JSON
        # save_dataset_to_json(pkl_file, "dataset/BX_test_preview.json")
    
    print("\n" + "="*80)
    print("ğŸ” å…¶ä»–å¯æŸ¥çœ‹çš„æ•°æ®é›†:")
    print("   - dataset/BX_train.pkl")
    print("   - dataset/BX_standard.pkl")
    print("   - dataset/FS_test.pkl")
    print("   - dataset/FS_train.pkl")
    print("   - dataset/FS_standard.pkl")
    print("="*80)

